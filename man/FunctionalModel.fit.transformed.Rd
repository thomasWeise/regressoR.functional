% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fitTransformed.R
\name{FunctionalModel.fit.transformed}
\alias{FunctionalModel.fit.transformed}
\title{Fit the Given Model Blueprint to the Specified Data}
\usage{
FunctionalModel.fit.transformed(metric, model, transformation.x = NULL,
  transformation.y = NULL, metric.transformed = NULL, par = NULL,
  fitters = FunctionalModel.fit.defaultFitters(base::length(metric@x),
  model@paramCount))
}
\arguments{
\item{metric}{an instance of
\code{regressoR.quality::RegressionQualityMetric}}

\item{model}{an instance of \code{\link{FunctionalModel}}}

\item{transformation.x}{the transformation along the \code{x}-axis, or
\code{NULL} if none was applied to the data}

\item{transformation.y}{the transformation along the \code{y}-axis, or
\code{NULL} if none was applied to the data}

\item{metric.transformed}{the transformed metric for the first fitting step}

\item{par}{the initial starting point}

\item{fitters}{the fitters}
}
\value{
On success, an instance of \code{\link{FittedFunctionalModel}}.
  \code{NULL} on failure.
}
\description{
Apply a set of fitters iteratively to fit the specified model to
the given data. First, we generate a starting guess about the
parameterization via \code{\link{FunctionalModel.par.estimate}} (or accept it
via the parameter \code{par}). From then on, we apply the different function
fitters one by one. All the fitters who have not produced the current best
solution are applied again, to the now-best guess. However, we do not apply
the fitters that have produced that very guess in the next round. (They may
get a chance again in a later turn.) Anyway, this procedure is iterated until
no improvement can be made anymore. After finishing the fitting, we attempt
whether rounding the fitted parameters to integers can improve the fitting
quality.
}
\examples{
set.seed(234345L)

orig.f.x.par <- function(x, par) exp(par[1] + par[2]*x - par[3]*x*x);
orig.par <- c(0.2, -0.3, 0.4);
orig.f.x <- function(x) orig.f.x.par(x, orig.par);
orig.x <- (-100:100)*0.05;
orig.y <- orig.f.x(orig.x);
noisy.x <- rnorm(n=length(orig.x), mean=orig.x, sd=0.05);
noisy.y <- rnorm(n=length(orig.y), mean=orig.y, sd=0.05*orig.y);

transformed.data <- dataTransformeR::TransformedData2D.new(
  dataTransformeR::Transformation.normalize(noisy.x),
  dataTransformeR::Transformation.log(noisy.y));

metric <- regressoR.quality::RegressionQualityMetric.default(noisy.x, noisy.y);
metric.transformed <- regressoR.quality::RegressionQualityMetric.default(transformed.data@x@data,
                                                 transformed.data@y@data);
model <- regressoR.functional.models::FunctionalModel.quadratic();
result <- FunctionalModel.fit.transformed(metric, model,
                                transformed.data@x@transformation,
                                transformed.data@y@transformation,
                                metric.transformed);

result.2 <- learnerSelectoR::learning.Result.finalize(result)
plot(noisy.x, noisy.y)
lines(noisy.x, result.2@f(noisy.x), col="red")
}
